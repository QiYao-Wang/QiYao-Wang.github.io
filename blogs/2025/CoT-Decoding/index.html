---
layout: compress
---
<!doctype html>
<html lang="en" class="no-js">

  <head>
    {% include head.html %}
    {% include head/custom.html %}
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
      html {
            scroll-behavior: smooth; /* 启用平滑滚动 */
        }
    </script>
  </head>
  <body>
    {% include browser-upgrade.html %}
    <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="https://qiyao-wang.github.io/">Homepage</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/blogs/">Blogs</a></li>
          </ul>
        </nav>
      </div>
    </div>
  </div>
    <div id="main" role="main">

      <div class="sidebar sticky">
        <div id="toc"></div>
      </div>

      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        {% if page.title %}<meta itemprop="headline" content="{{ page.title | markdownify | strip_html | strip_newlines | escape_once }}">{% endif %}
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">

            <h1 id="cmu-dlsys-course-homework-0:-implementation-and-reflection">Chain-of-Thought Reasoning without Prompting</h1>
            <p style="color:gray">Jan. 07, 2025 - Jan. XX, 2025 · Qiyao Wang · Topic #Reasoning</p>

              <p>大模型的推理最近很火，我个人对于推理的理解是基于线索或前面的推理逻辑不断向后有逻辑地延伸，这或许和自回归的形式很像，Denny Zhou 的这篇论文<sup>[1]</sup>通过修改贪婪解码（greedy decoding）为基于 CoT-Decoding 的有选择性的 top-k decoding，从无需提示（without prompting）的视角来探索或观察大模型内部的推理。本文将首先详细介绍这篇论文，之后进行代码实现，希望能在一周写完 😁。</p>

              <h1>论文精读与思考</h1>
              <h2>The Introduction of Research Question</h2>
              <p>在本文之前，大部分的论文聚焦在利用 Prompting 的方式来激发 LLMs 的推理能力，如 ICL 中的 Few-Shot Learning 在测试问题前给定几个与测试问题类似的示例，这些示例中通常包含着对应问题解决方案的推理流程，概括的说即<strong>给定具有中间步骤的少样本提示（Few-Shot）</strong>，或使用零样本（Zero-Shot）提示的方式指导模型给出相应的中间步骤，例如 (Nye et.al, 2021<sup>[2,3]</sup>） 在 LLMs 出现之前就尝试使用 XML 格式来指导模型在给出正确答案之前在 <code>&lt;scratch&gt; computing process &lt;/scratch&gt;</code> 中给出计算过程，之后再给出答案，以及利用显式的思维链提示<sup>[4]</sup> <code>Let's think step by step.</code> 激发模型的推理能力，使得大模型在输出最终答案之前能够给出一定的推理过程。除 prompting 方法外，还可以通过 model training 或 instruction tuning 来激发模型的推理能力，但这样的方式通常会使得模型在通用能力上产生灾难性遗忘的效果，在数据层面，基于参数训练的方法还需要构造大量的 CoT 的推理数据，成本较高。</p>

              <p>提示方法的关键是如何在大模型回答最终答案之前给出一定的推理中间过程（intermediate steps），这个中间过程是推理的关键。除提示方法外，也有利用 model training 和 instruction tuning 来提升模型生成中间步骤能力的方法，但是这样的方法与基于 prompt 的方法，都受限于 “instruction/prompt”，有时候需要不断地修缮你精心设计的 prompt 才有可能提升一些性能。本文提出了这样的问题，<strong>在无需精心设计提示的情况下，如何激发LLMs的推理能力？</strong>，以及 <strong>如何更好地理解模型自身的内部推理能力？</strong> 但大模型通常倾向于直接输出最终的问题结果，本文从解码过程出发，提出了<strong>CoT-decoding</strong>，能够不基于提示，激发模型的推理能力（生成中间的推理步骤）。</p>

              <h2>CoT-Decoding Methodology</h2>
              <p>为了在无需 prompting 的前提下激发模型的推理能力，本文从 decoding 阶段出发，先不考虑包含 top-k、top-p 及 temperature 在内的采样方法，大语言模型可以通过 greedy decoding 的方式来采样，即每次选择出现概率最大的 Token。本文的基本思路是，在第一个解码步（first decoding step）时，不仅只采样 greedy decoding 对应的 Token，而是引入超参数 k，选择第一个解码步中 top-k 个 Tokens，在后续的解码步中，分别对该 k 个 Token 进行贪婪解码，如图 1 所示。</p>
              <figure>
                <img src="Illustration_of_CoT-decoding.jpg" alt="Illustration_of_CoT-decoding-image">
                <figcaption>图1：CoT-Decoding 解释</figcaption>
              </figure>
              <p>在获得第一个解码步的 k 个 Tokens 对应的 CoT-Path 后，一个很自然的问题是：<strong>如何选择正确的/恰当的 CoT-Path？</strong>在 Self-Consistency<sup>[5]</sup> 中通过聚合采样后的答案，选出其中投票次数（出现最多）的答案作为最终的结果，这是从一致性角度看来十分自然的。本文的 CoT-Decoding 为每条 CoT-Path 引入了置信度（confidence），其中关注每个解码步骤，对于 第 $k$ 个 CoT-Path 的置信度计算如下</p>
              $$
              \delta_{k,\text{answer}} = \frac{1}{|\text{answer}|}\sum_{x_t\in\text{answer}}p(x_t^1|x_{< t})-p(x_t^2|x_{< t})
              $$

              <p>即，在给定前 $t-1$ 个 Tokens 解码第 $t$ 步时，第 $t$ 步中概率最大的两个 Token $x_t^1$ 和 $x_t^2$ 的差值，从该 CoT-Path 的第一个 Token 到结尾 Token 的该差值之和的归一化即为置信度。这样的定义能够得到所有解码步中采样该 Token 的平均置信度，但是我认为（作者在结论中也有提到）这种置信度计算方法似乎不太准确，因为并不是所有步的置信度都是同等重要。通过该置信度的计算方式，所有的 $k$ 个解码路径都能够被对应的 $\Delta$ 值进行标记。图 1 中右下角的图例可以看到颜色越深则代表该 CoT-Path 的 confidence $\Delta$ 越大，甚至这样的指标在该问题示例上与结果的正确性有一定的相关性，即在正确答案上的置信度大于错误答案；与 greedy decoding 这种没有 CoT-Path 的采样结果相比，CoT-Path 具有更大的 $\Delta$。同时作者阐明，在 $k=10$ 的 top-10 解码路径中抽取最大置信度的路径的实验中，88% 的数据都存在 CoT-Path。如图 2 所示，为 PaLM-2 模型在 GSM8K 和 Year Parity 数据集上的示例，其中括号中的蓝色数字即表示对应 CoT-Path 的置信度。</p>
              <figure>
                <img src="example-palm.jpg" alt="example-palm-image">
                <figcaption>图2：PaLM 模型在 GSM8K 和 Year Parity 数据集上的示例</figcaption>
              </figure>
              <h2>Findings</h2>
              <p>本文不过多阐释具体的实验内容及数据集等，而是以作者报告的相关发现为脉络对实验结果进行展示与分析。</p>

              <h1>代码复现</h1>

              <h1>Reference</h1>
              <p>[1] Wang X, Zhou D. <a href="https://openreview.net/forum?id=4Zt7S0B0Jp">Chain-of-Thought Reasoning Without Prompting</a> [C]. The Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
              <p>[2] Nye M, Andreassen AJ, Gur-Ari G, et al. <a href="https://openreview.net/forum?id=HBlx2idbkbq">Show Your Work: Scratchpads for Intermediate Computation with Language Models</a> [C]. ICLR, Deep Learning for Code Workshop, 2022.</p>
              <p>[3] Blog: <a href="https://leehanchung.github.io/blogs/2024/10/21/reasoning-inference-scaling/">https://leehanchung.github.io/blogs/2024/10/21/reasoning-inference-scaling/</a></p>
              <p>[4] Wei J, Wang X, Schuurmans D, et al. <a href="https://arxiv.org/abs/2201.11903">Chain-of-thought prompting elicits reasoning in large language models</a>[J]. Advances in neural information processing systems, 2022, 35: 24824-24837.</p>
              <p>[5] Wang X, Wei J, Schuurmans D, et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models[C]. The Eleventh International Conference on Learning Representations.</p>

              <h1>Contact</h1>
              <p>There may be some errors present. If you find any, please feel free to contact me at <code>wangqiyao@mail.dlut.edu.cn</code>. I would appreciate it!</p>

          </section>
        </div>
      </article>
    </div>
    {% include scripts.html %}
    <script>
        // 动态生成目录
        const toc = document.getElementById('toc');
        toc.innerHTML = '<strong>Table of Contents</strong>';

        const ul = document.createElement('ul');
        toc.appendChild(ul);

        // 获取所有标题
        const headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
        headers.forEach(header => {
            const li = document.createElement('li');
            li.style.marginLeft = `${(parseInt(header.tagName[1]) - 1) * 10}px`;

            const a = document.createElement('a');
            a.href = `#${header.id || header.innerText.replace(/\s+/g, '-').toLowerCase()}`;
            a.textContent = header.innerText;
            a.target="_self";

            if (!header.id) {
                header.id = header.innerText.replace(/\s+/g, '-').toLowerCase();
            }

            li.appendChild(a);
            ul.appendChild(li);
        });
    </script>
  </body>
</html>
