---
layout: compress
---
<!doctype html>
<html lang="en" class="no-js">

  <head>
    {% include head.html %}
    {% include head/custom.html %}
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
      html {
            scroll-behavior: smooth; /* 启用平滑滚动 */
        }
    </script>
  </head>
  <body>
    {% include browser-upgrade.html %}
    <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="https://qiyao-wang.github.io/">Homepage</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/blogs/">Blogs</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/paper_daily/">Paper Daily</a></li>
          </ul>
        </nav>
      </div>
    </div>
  </div>
    <div id="main" role="main">

      <div class="sidebar sticky">
        <div id="toc"></div>
      </div>

      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        {% if page.title %}<meta itemprop="headline" content="{{ page.title | markdownify | strip_html | strip_newlines | escape_once }}">{% endif %}
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">

            <h1 id="RL_Basic_Knowledge_Before_PPO">Basic Knowledge of Reinforcement Learning before PPO</h1>
            <p style="color:gray">Feb. 23, 2025 -  · Qiyao Wang · Topic #RL</p>
              <p>Claim：本文参考 <a href="https://newfacade.github.io/notes-on-reinforcement-learning/03-approach.html">notes on Reinforcement Learning</a> 和 <a href="https://www.youtube.com/playlist?list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_">Hung-yi Lee</a> 总结记录学习，记录难点和分析，最基础的内容不赘述。</p>

              <h2>Basic Concept</h2>
              <p><strong>What's the Reinforcement Learning: A formal definition</strong></p>
              <p>Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.</p>
              <p>Translation: 强化学习是一种 通过构建 与环境交互并从试错中学习的代理来解决控制任务（也称为决策问题）的框架。代理通过试错与环境交互，并根据其行为收到正向或负向的奖励作为唯一的反馈。</p>

              <p><strong>Difference between Observations and States</strong></p>
              <p>Observations 和 States 都表示 Agent 从环境中获取的信息，在概念上二者几乎一致，仅在信息的覆盖范围上存在差异，在实现上一般会进行区分。二者的区别如下：</p>
              <ul>
                <li>State $s$: is a <strong>complete</strong> description of the state of the world (there is no hidden information). 全部信息，无隐藏内容。</li>
                <li>Observation $o$: is a <strong>partial</strong> description of the state. 部分信息。</li>
              </ul>

              <h2><strong>Two main approaches for solving RL problems</strong></h2>
              <p>RL 的目标是得到一个好的 Agent，即得到一个最优的 Policy Model $\pi^*$，能够根据当前的环境的 State 做出良好的决策。Policy Model $\pi$ 的优化目标是最大化期望回报（expected return）。</p>
              <p>训练 Agent 得到最优 policy $\pi^*$ 的方法有两种：</p>
              <ul>
                <li>Policy-Based Methods: 直接地训练模型在给定 State 的情况下，选择适合的 Action。</li>
                <li>Value-Based Methods: 训练模型能够判断更有价值的 State，并且采取适合的 Action，来达到更有价值的 State。</li>
              </ul>

              <h3>Policy-Based Methods</h3>
              <p>对于 Policy-Based Methods，其目标是直接学一个 policy function。对于<strong> Action 是离散的，</strong> policy model $\pi$ 给定 State 返回适合的 Action</p>
              $$
              a_t = \pi(s_t)
              $$
              <p>对于<strong> Action 是连续的</strong> policy model $\pi$ 实际上返回的是一个 Action distribution，适合的 Action 应该能够从该分布中被采样出来</p>
              $$
              a_t\sim\pi(\cdot|s_t)
              $$

              <h3>Value-Based Methods</h3>
              <p>对于 Value-Based Methods，其目标是能够预测在某一 State $s$ 下的 accumulated expected return （state-value function $V_{\pi}(s)$）或某一 State $s$ 下采取 Action $a$ 后的 accumulated expected return (action-value function $Q_{\pi}(s,a)$)。</p>
              <h4>The state-value function</h4>
              $$
              V_{\pi}(s) = \mathbb{E}_{\pi}[G_t|S_t=s]
              $$
              <p>state-value function 的含义是估计在 State $s$ 的条件下，累积折扣回报 $G_t=\sum_{k=0}^∞\gamma^kR_{t+k+1}$ 的期望值大小。</p>

              <h4>The action-value function</h4>
              $$
              Q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]
              $$
              <p>action-value function 实际上是 state-action-value function，即在给定 State $s$ 和 Action $a$ 的条件下，累积折扣回报 $G_t$ 的期望值大小。由此可见，Value-Based Method 的估计目标都是：$G_t$，即累积折扣回报的期望值大小。</p>

            <h3>Relation between V and Q Function</h3>
            <p>从定义看，$V_\pi$ 和 $Q_\pi$ 都是对累积折扣回报 $G_t$ 的期望值估计，但是二者所站角度不同。$Q_\pi$ 相较于 $V_\pi$ 而言，要考虑 Action $a$ 条件的影响，而 $V_\pi$ 则不关心 Action，从 $Q_\pi$ 得到 $V_\pi$ 的公式如下</p>
            $$
            V_{\pi}(s)=\sum_{a\in\mathcal A}\pi(a|s)Q_\pi(s,a)
            $$
            <p>即，对 $Q_\pi$ 中的 Action 进行期望的计算，排除其中选择某一 Action 的影响，而是在 Action Space 下的期望值。而 $Q_\pi(s,a)$ 中的 $a$ 其实也仅考虑了一步 State $s$ 下的 Action，对于之后的 State 并不强制其 Action，可借此与 $V_\pi$ 关联</p>
            $$
            Q_\pi(s,a) = \sum_{s'\in\mathcal S}P(s'|s,a)[R(s,a,s')+\gamma V_\pi(s')]
            $$
            <p>其中，State $s$ 和 该状态下的 Action $a$ 固定，而后续的 State 不确定，会从 State Space $\mathcal{S}$ 中进行采样，因此对于 State $s$ 采取 Action $a$ 后的下一个状态 $s'$ 需要结合<strong>状态转移函数</strong> $P(s'|s,a)$ 进行计算。$R(s,a,s')$ 则为从 State $s$, Action $a$ 转移为 State $s'$ 后环境给予的 Reward 值，$V_\pi(s')$ 则表示在 State $s'$ 条件下，未来的累积折扣期望值。</p>

            <h3>Bellman Equation</h3>
            <p>Bellman Equation 与上述从 $V_\pi$ 得到 $Q_\pi$ 的公式较为类似，以一种动态规划/递归的形式，将在当前状态下的 expected return 拆解为 当前立即得到的 Reward 与对未来（下一状态）的 expected return 的估计。</p>
            <p><strong>Bellman Equation of the state-value function</strong></p>
            $$
            V_\pi(s)=\mathbb{E}_\pi[R_{t+1}+\gamma*V_\pi(S_{t+1})|S_t=s]
            $$
            <p><strong>Bellman Equation of the action-value function</strong></p>
            $$
            Q_\pi(s,a)=\mathbb{E}_\pi[R_{t+1}+\gamma*Q_{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]
            $$
            <h4>Closed-form Solution</h4>
            <p>上述方程，无论是对于 $V$ 还是 $Q$ Function 的思想都是一致的，Bellman Equation 为这两类 Value Function 提供了简化计算的方法。将 $V$ 与 $Q$ 关系中的式子融合，可以得到</p>
            $$
            V_\pi(s)=\sum_{a\in\mathcal A}\pi(a|s)\sum_{s'\in\mathcal S}P(s'|s,a)[R(s,a,s')+\gamma V_\pi(s')]
            $$
            <p>将上面的公式转化为矩阵形式，可以先看一下每个元素的含义，其中 $P(s'|s,a)$ 为状态转移的概率，$R(s,a,s')$ 为当前状态下的 reward，那么下面将这些元素矩阵化，将有以下内容</p>
            <ul>
              <li>转移概率矩阵 $P_\pi\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}$： ${P}_\pi$ 代表在给定策略 $\pi$ 下，从状态 $s$ 到状态 $s'$ 的转移概率。它是通过对所有可能的动作 $a$ 进行加权求和得到的。
                $$
                P_\pi(s,s')=\sum_{a\in\mathcal A}\pi(a|s)\cdot P(s'|s,a)
                $$
              </li>
              <li>奖励向量 $\mathbf{r}_\pi\in\mathbb{R}^{|\mathcal{S}|\times1}$：表示在状态 $s$ 下，按照策略 $\pi$ 选择动作，经过状态转移得到的即时奖励的期望值
                $$
                R_\pi(s) = \sum_{a\in\mathcal A}\sum_{s'\in\mathcal{S}}\pi(a|s)P(s'|s,a)R(s,a,s')
                $$
              </li>
            </ul>
            <p>因此，矩阵形式为</p>
            $$
            \mathbf{v}_\pi=\mathbf{r}_\pi+\gamma P_\pi\mathbf{v}_\pi
            $$
            <p>经过矩阵运算，当 $I-\gamma P_\pi$ 可逆时，$\mathbf{v}_\pi=(I-\gamma P_\pi)^{-1}\mathbf{r}_\pi$ 为 Bellman Equation 的唯一解。</p>
            <p>基于 <a href="https://newfacade.github.io/notes-on-reinforcement-learning/04-bellman.html">Gershgorin circle theorem</a> 可以证明 $I-\gamma P_\pi$ 可逆，此处略，因此 Bellman Equation 具有唯一的闭合解，这为之后的基于对 Bellman Equation 优化的方法提供了基础。</p>

            <h4>Optimal action-value</h4>
            <p>基于贝尔曼最优方程 (<a href="https://newfacade.github.io/notes-on-reinforcement-learning/05-bellman-optimal.html">The Bellman Optimality Equation, BOE</a>)，能够推导出最优的价值函数的形式解，推导最优的 policy model $\pi^*$，也就是说，BOE 的解，对应了最优的 state-value $V^*$ 和 policy $\pi^*$，基于这两者能够得到最优的 action-value $Q^*$。</p>
            $$
            \begin{aligned}
            V^*(s)&=\max_aQ^*(s,a)\\
            &=\max_a\sum_{s'\in\mathcal{S}}P(s'|s,a)[R(s,a,s')+\gamma V^*(s')]
            \end{aligned}
            $$

            $$
            \begin{aligned}
            Q^*(s)&=\sum_{s'\in\mathcal{S}}P(s'|s,a)[R(s,a,s')+\gamma V^*(s')]\\
            &=\sum_{s'\in\mathcal{S}}P(s'|s,a)[R(s,a,s')+\gamma \max_aQ^*(s,a)]
            \end{aligned}
            $$

            <p><strong style="color: darkred">第一次了解 Bellman Equation，此部分逻辑不清，需梳理。</strong></p>

              <h1>Reference</h1>

              <h1>Contact</h1>
              <p>There may be some errors present. If you find any, please feel free to contact me at <code>wangqiyao@mail.dlut.edu.cn</code>. I would appreciate it!</p>

          </section>
        </div>
      </article>
    </div>
    {% include scripts.html %}
    <script>
        // 动态生成目录
        const toc = document.getElementById('toc');
        toc.innerHTML = '<strong>Table of Contents</strong>';

        const ul = document.createElement('ul');
        toc.appendChild(ul);

        // 获取所有标题
        const headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
        headers.forEach(header => {
            const li = document.createElement('li');
            li.style.marginLeft = `${(parseInt(header.tagName[1]) - 1) * 10}px`;

            const a = document.createElement('a');
            a.href = `#${header.id || header.innerText.replace(/\s+/g, '-').toLowerCase()}`;
            a.textContent = header.innerText;
            a.target="_self";

            if (!header.id) {
                header.id = header.innerText.replace(/\s+/g, '-').toLowerCase();
            }

            li.appendChild(a);
            ul.appendChild(li);
        });
    </script>
  </body>
</html>
