---
layout: compress
---
<!doctype html>
<html lang="en" class="no-js">

  <head>
    {% include head.html %}
    {% include head/custom.html %}
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
      html {
            scroll-behavior: smooth; /* 启用平滑滚动 */
        }
    </script>
  </head>
  <body>
    {% include browser-upgrade.html %}
    <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="https://qiyao-wang.github.io/">Homepage</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/blogs/">Blogs</a></li>
          </ul>
        </nav>
      </div>
    </div>
  </div>
    <div id="main" role="main">

      <div class="sidebar sticky">
        <div id="toc"></div>
      </div>

      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        {% if page.title %}<meta itemprop="headline" content="{{ page.title | markdownify | strip_html | strip_newlines | escape_once }}">{% endif %}
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">

            <h1 id="cmu-dlsys-course-homework-0:-implementation-and-reflection">CMU DLSys Course Homework 1: Implementation and Reflection</h1>
            <p style="color:gray">Jan. 08, 2025 - Jan. XX, 2025 · Qiyao Wang · Topic #MLSys</p>

            <p>本周看完了课程的 Lecture 3/4/5，这几节课程是对自动求导（Automatic Differentiation）的讲解，其中反向传播、计算图是重点。本文针对 <a href="https://github.com/dlsyscourse/hw1">Homework 1</a> 进行实践与思考。</p>

            <h1>Introduction of Homework 1</h1>
            <p>通过 Homework 1 自己实践构建一个 needle（Necessary Elements of Deep Learning） 库，其中本文最关键的是自动求导框架的构建，并在 <a href="https://qiyao-wang.github.io/blogs/2025/CMU_DLSys_Hw0/">Homework 0</a> 中的双层神经网络上进行应用。</p>
            <p>本文对于 needle 的构建是建立在 numpy 库和 CPU 基础上的，后续的作业也会实现相应的底层多维数组以及应用到 GPU 节点上。</p>
            <p>Homework 1 的 needle 库包含两个重要的文件，即 autograd.py 和 ops_mathematic.py，其中前者定义了计算图框架以及自动求导框架，后者包含在计算图中会用到的运算符（compute function），及运算符对应的求导函数（gradient function）。</p>
            <h2>Basic Concept of Needle</h2>
            <p>本文中应用 needle 库中最重要的几个类包括张量 Tensor 和其继承的父类 Value，以及对应的张量运算符 TensorOp 和其继承的父类 Op。下面对这几个类进行详细解释</p>
            <ul>
              <li>
                <p><strong>Value</strong> (the most abstract class)</p>
                <p>Value 类是抽象程度最高的类，其实例化的每一个对象都作为计算图中的一个节点（甚至可直接认为其对象代表一个计算图，因为可通过节点进行轨迹追溯）。其中包含四个重要的部分：<code>op: Optional[Op]</code>，该语句定义了节点 Value 所链接的其他节点进行的操作，如假设 $v_3=v_1+v_2$，则 op 中存储的是 ops_mathematic.py 中定义的 “+”（EWiseAdd 逐元素加法），其中 Optional 代表 op 的数据类型要么是 Op Class，要么是 None；<code>inputs: List["Value"]</code>，其中包含了计算图中与当前节点相关的节点，在 $v_3=v_1+v_2$ 的例子中，$v_3$ 的 inputs 则为 $[v_1,v_2]$，通过 op 和 inputs 可以有效地获得对应计算图的完整轨迹；<code>cached_data: NDArray</code>，则是该节点存储的具体的值，在本作业中还是使用 numpy 中的多维数组来简化作业；<code>requires_grad: bool</code>，则是对该节点是否需要求导，当值为 False 时，表示该节点无需求导，无需进行梯度下降的优化。</p>
              </li>
              <li>
                <p><strong>Tensor</strong> (the subclass of value and the interface with user)</p>
                <p>Tensor 类是 Value 的子类，并且对应一个实际的张量节点（计算图中的一个多维数组），计算图中实际应用 Tensor 类而非 Value 类。</p>
              </li>
              <li>
                <p><strong>Op</strong> (Operations)</p>
                <p>Op 类用于构建计算图中包含的具体的运算符，每一个运算符需要定义计算的前馈函数 <code>compute()</code> 和该运算符对应的求导过程 <code>gradient()</code>，用于后续的反向传播。</p>
              </li>
              <li>
                <p><strong>TensorOp</strong> (the subclass of Op)</p>
                <p>TensorOp 类是 Op 的子类，它继承了 Op 中的所有操作符，并且能够返回 Tensor 类型的结果。</p>
              </li>
            </ul>
            <p>在实际运行 needle 库时，需要使用的是 Tensor 类，而 TensorOp 等类都内置于 Tensor 中，以 $c = a + b$ 为例</p>
<pre>
<code class="language-python">import needle as ndl
a = ndl.Tensor([1], dtype="float32")
b = ndl.Tensor([2], dtype="float32")
c = a + b # c: Tensor([3], dtype="float32")</code>
</pre>
            <p>上述代码的实际运算过程如下：
              <ol>
                <li><code>Tensor.__add__</code> 调用 Tensor 类中的 "__add__" 方法，由于运算符两侧均为 Tensor，则实际运行的是 Op 中的逐元素加法 <code>EWiseAdd()</code>；</li>
                <li><code>TensorOp.__call__</code>；</li>
                <li>Tensor.make_from_op(op: Op, inputs: List["Value"])</code> 首先为 c 实例化了一个新的 Tensor 对象，而非在 a/b 上进行修改，将对应的运算符和参与运算的节点传入新创建的对象 c 中进行初始化，其中需要对 <strong>LAZY_MODE</strong> 进行判断，最终返回新的 Tensor 对象 c；</li>
                <li>在返回之前，经过了 <code>Value.realize_cached_data()</code> 或 <code>Tensor.detach()</code> 的计算，二者区别（及 LAZY_MODE）将在后面详细说明；</li>
                <li>进入 Op 类的前馈计算函数中 <code>EWiseAdd.compute</code> 才能得到第 3 步中返回的新 Tensor 对象 c。</li>
              </ol>
            </p>

            <h3>LAZY_MODE and Detach Function</h3>
            <p>如下方的代码块所示，为 Value 类中的成员函数 <code>make_from_op()</code>，其中包含了这两个元素</p>
<pre>
<code class="language-python">@classmethod
def make_from_op(cls, op: Op, inputs: List["Value"]):
  value = cls.__new__(cls) # create a new object of Value Class
  value._init(op, inputs) # initialize object
  if not LAZY_MODE:
    if not value.requires_grad:
      return value.detach()
    value.realize_cached_data()
  return value</code>
</pre>
            <p>LAZY_MODE 用来控制计算立即执行还是延迟执行。当设置 <code>ndl.autograd.LAZY_MODE = True</code> 时，运行 $c = a + b$ 时并不会立刻进行 <code>realize_cached_data()</code> 的计算，此时的 <code>c.cached_data is None == True</code>，这样只构建计算图但不进行计算，能够有效地优化内存占用，当需要使用 $c$ 的数值时，可以利用 <code>c.data()</code> 来通过前馈计算图获得该节点的 cache_data 数值。</p>
            <p>Detach function 用来控制节点是否与计算图链接。如下方的代码所示，虽然看起来简单应该不会占用太多内存，但是由于 Tensor 的计算实质上是对计算图的构建，如图 1 所示，在新节点建立时，背后其实构建了新的计算图，当计算图过多如在迭代中，容易内存/显存溢出。通过 detach 将返回一个新的 Tensor 节点，这个节点孑然一身与其他计算图不相连，即不保留计算图的轨迹。而 detach 是在 <code>requires_grad=False</code> 时才调用的，正因无需计算梯度所以可以只保存为一个单一节点。</p>
<pre>
<code>for i in range(100):
  sum_loss = sum_loss + x * x</code>
</pre>
              <figure>
                <img src="detach.png" alt="detach function">
                <figcaption>图1：detach 函数示例：$y=x+1$</figcaption>
              </figure>

            <h1>Question 1 & 2: Implementing forward and backward computation</h1>
            <p>在 needle 中，计算图通过 TensorOp 类下属的各个子类进行实际的张量运算，每一个运算符都具有 compute 成员函数，负责前馈计算，本节的多维数组无需自行构建，而是使用 numpy 的 NDArray 实现，为了后续的移植，以 <code>import numpy as array_api</code> 的形式导入 numpy 包。下面对运算符进行依次实现，在本节的代码中将包含每个运算符类的初始化函数（__init__）、前馈函数（compute）以及求导函数（gradient）。</p>
            <p>前馈计算直接按照各运算符的定义执行即可，但每个运算符的求导都较为复杂，需要首先明确的是，该求导的对象是损失函数 loss function $\ell$ 对当前节点 $v_{i+1}$ 的输入节点 $v_i$ 的导数（此处假设 $v_{i+1}$ 仅有一个输入，即 $v_i$，当具有多个输入时，分别计算偏导数即可），$v_{i+1}$ 是其 inputs $v_i$ 在 对应运算符上的函数，即 $v_{i+1}=g(v_{i})$，其中 $\text{out_grad} = \frac{\partial \ell}{\partial v_{i+1}}$，那么导数为</p>
              $$
              \frac{\partial \ell}{\partial v_{i}}=\frac{\partial \ell}{\partial v_{i+1}}\cdot\frac{\partial v_{i+1}}{\partial v_i}=\text{out_grad}\cdot\frac{\partial v_{i+1}}{\partial v_{i}}=\text{out_grad}\cdot g'(v_i)
              $$
            <p>gradient function 会返回的是针对 inputs 中每个节点的偏导数元组。</p>
            <ul>
              <li>
                <p><strong>PowerScalar</strong>：对张量进行乘方运算 $X^n$，例如 $X^2=XX^T$，numpy中提供了相应的乘方接口，直接调用即可</p>
                <p>下面详细推导张量乘方运算的求导函数，假设计算式为 $v_2=v_1^n$，则 loss function 对 $v_2$ 的导数可计算为</p>
                $$
                \frac{\partial\ell}{\partial v_1}=\frac{\partial\ell}{\partial v_2}\cdot\frac{\partial v_2}{\partial v_1}=\text{out_grad}\cdot nv_1^{n-1}
                $$
<pre>
<code class="language-python">class PowerScalar(TensorOp):
  def __init__(self, scalar:int):
    self.scalar = scalar

  def compute(self, a: NDArray) -> NDArray:
    return array_api.power(a, self.scalar)

  def gradient(self, out_grad, node):
    a = node.inputs
    return out_grad * self.scalar * array_api.power(a, self.scalar - 1)</code>
</pre>
              </li>
              <li>
                <p><strong>EWiseDiv</strong>：EWise 开头的函数均是<strong>逐元素（elementwise）</strong>的运算，该函数是对两个张量进行逐元素运算，即 $A \oslash B$</p>
                <p>该函数需要考虑分类讨论三种情况：</p>
                  <ol>
                      <li>张量 $A$ 和张量 $B$ 的形状（维度）不同，逐元素的运算要求两张量形状相同；</li>
                      <li>张量 $B$ 的元素全为 0，虽然在 numpy 中会考虑除 0 的情况，numpy会返回 inf/-inf，符号取决于被除数的符号，在本情况下为 $B$ 的所有元素均为 0；</li>
                      <li>正常情况或张量 $B$ 的元素部分为 0，见上一条的解释。</li>
                  </ol>
                <p>下面详细推导逐元素除法函数的导函数。由于逐元素，因此可以从单个元素的角度来进行计算，其中会使用类似标量的求导法则，对 $v_3=\frac{v_2}{v_1}$ 如下式所示</p>
                $$
                \frac{\partial\ell}{\partial v_1}=\frac{\partial \ell}{\partial v_3}\cdot\frac{\partial v_3}{\partial v_2}=\text{out_grad}\circ(v_2 \circ (-1) \oslash (v_1\circ v_1))
                $$
                $$
                \frac{\partial\ell}{\partial v_2}=\frac{\partial \ell}{\partial v_3}\cdot\frac{\partial v_3}{\partial v_2}=\text{out_grad}\circ(1\oslash v_1)
                $$
                <p>其中 $v_2$ 为 lhs (left hand side)，$v_1$ 为 rhs (right hand side)，在下面的运算中均为逐元素的计算，如 numpy 中 * 为逐元素，@ 为矩阵乘法，分别返回对 lhs 和 rhs 的偏导数</p>
<pre>
<code class="language-python">class EWiseDiv(TensorOp):
  def compute(self, a, b):
    if a.shape != b.shape:
      raise RuntimeError("The shapes of the two tensors are inconsistent.")
    if b.all() == 0:
      raise RuntimeError("All elements of the divisor tensor are zero.")
    return array_api.divide(a, b)

  def gradient(self, out_grad, node):
    lhs, rhs = node.inputs
    return out_grad / rhs, out_grad * lhs / rhs / rhs * (-1)</code>
</pre>
              </li>
              <li>
                  <p><strong>DivScalar</strong>：张量逐元素除标量，如 $v_2=v_1/a$，同样需要判断除数是否为 0。numpy 的 divide 函数会自动判别除数的类型，其实在除以标量时，实际上是将标量广播（broadcast）为复制为与被除张量相应维度的矩阵进行逐元素的除法。导函数为</p>
                  $$
                  \frac{\partial \ell}{\partial v_1}=\frac{\partial\ell}{\partial v_2}\cdot\frac{\partial v_2}{\partial v_1}=\text{out_grad}\cdot\frac{1}{a}
                  $$
<pre>
<code class="language-python">class DivScalar(TensorOp):
  def __init__(self, scalar):
    self.scalar = scalar

  def compute(self, a):
    if self.scalar == 0:
      raise RuntimeError("The divisor is zero.")
    return array_api.divide(a, self.scalar)
  def gradient(self, out_grad, node):
    return out_grad / self.scalar</code>
</pre>
              </li>
              <li>
                  <p><strong>MatMul</strong>：矩阵乘法 $AB$，numpy 在其中的矩阵乘法函数中已经考虑了两个矩阵的形状的约束。</p>
                  <p>对于该函数的导数，假设 $v_3 = v_1 * v_2$，即 lhs 为 $v_1$，rhs 为 $v_2$，那么</p>
                  $$
                  \frac{\partial \ell}{\partial v_1}=\frac{\partial \ell}{\partial v_3}\cdot\frac{\partial v_3}{\partial v_1} = \text{out_grad}\cdot v_2^T
                  $$
                  $$
                  \frac{\partial \ell}{\partial v_2}=\frac{\partial \ell}{\partial v_3}\cdot\frac{\partial v_3}{\partial v_2} = \text{out_grad}\cdot v_1^T
                  $$
                  <p>这里需要考虑形状，假设 $v_3\in\mathbb{R}^{p\times q}, v_1\in\mathbb{R}^{p\times s}, v_2\in\mathbb{R}^{s\times q}$，out_grad 的形状与 $v_3$ 一致，即 $\mathbb{R}^{p\times q}$，则 $\frac{\partial \ell}{\partial v_1}=v_2^T\in\mathbb{R}^{q\times s}$，$\frac{\partial \ell}{\partial v_2}=v_1^T\in\mathbb{R}^{s\times p}$，则 <code>dlhs=matmul(out_grad, rhs.T)</code> 形状为 $\mathbb{R}^{q\times s}$ ，<code>drhs=matmul(lhs.T, outgrad)</code> 形状为 $\mathbb{R}^{s\times p}$，此时符合导数与原函数的维度相同。</p>
                  <p>需要注意的是，上述没有考虑在前向传播的过程中使用<strong>广播机制</strong>，dlhs 的维度可能会比 lhs 的维度多一些，为了使维度统一，需要对这些多余的维度进行求和。例如假设 dlhs.shape = (4, 3, 2)，而 lhs.shape = (3, 2)，那么广播机制在最左侧（通过计算动态得知加入的维度位置）加入了一个维度，需要通过 reduce（实际使用下面的 summation）来减少维度使得维度统一。</p>
<pre>
<code class="language-python">class MatMul(TensorOp):
  def compute(self, a, b):
    return array_api.matmul(a, b)

  def gradient(self, out_grad, node):
    lhs, rhs = node.inputs
    dlhs = array_api.matmul(out_grad, rhs.T)
    drhs = array_api.matmul(lhs.T, out_grad)

    if dlhs.shape != lhs.shape:
      dlhs = summation(dlhs, tuple(range(len(dlhs.shape) - len(lhs.shape))))
    if drhs.shape != rhs.shape:
      drhs = summation(drhs, tuple(range(len(drhs.shape) - len(rhs.shape))))
    assert (dlhs.shape == lhs.shape)
    assert (drhs.shape == rhs.shape)
    return dlhs, drhs</code>
</pre>
                <p>其中 <code>tuple(range(len(dlhs.shape) - len(lhs.shape)))</code>  动态确定需要求和的维度索引，适用于各种形状的广播情况。例如上面的例子，len(dlhs.shape) - len(lhs.shape) = 1，则 range(1) 为 [0]，tuple 为 (0, )，表示聚合 dlhs 最左侧的维度。通过这样的计算，可以动态获取聚合的轴，从而使得形状对齐。</p>
              </li>
              <li>
                  <p><strong>Summation</strong>：计算给定轴的和，其中 axes 有多种情况。</p>
                  <ol>
                      <li>axes 为单个整数，即按指定轴求和（0（默认值） 为列，1 为行）；</li>
                      <li>axes 为元组，如 axes=(0,1)，则沿指定的多个轴求和，在矩阵中此时为求矩阵所有元素的和；</li>
                      <li>axes 为 None，即求矩阵的所有元素的和</li>
                  </ol>
                  <p>对于 summation 的导函数，假设输入张量为 $v_i=\mathbb{R}^{n_1\times n_2\times\cdots\times n_k}$，其中对某些维度 axes 进行求和，即 $v_{i+1}=\text{summation}(v_i, \text{axes})$，当 $\text{axes}={d_1,d_2,\cdots,d_m}$时，输出张量的形状应为 $v_{i+1}$.shape$=(n_1,n_2,\cdots,n_k)\text{\}(n_{d_1},n_{d_2},\cdots,n_{d_m})$，即</p>
                  $$
                  v_{i+1} = \sum_{d_1,d_2,\cdots,d_m}v_i[\cdots,d_1,d_2,\cdots,d_m,\cdots]
                  $$
                  <p>其中 $d_1,d_2,\cdots,d_m$ 为被求和维度的索引。导函数的目标为求得 $\frac{\partial \ell}{\partial v_i}$，具体推导如下</p>
                  $$
                  \frac{\partial\ell}{\partial v_i}=\frac{\partial \ell}{\partial v_{i+1}}\cdot\frac{}{}
                  $$
<pre>
<code class="language-python">class Summation(TensorOp):
  def __init__(self, axes: Optional[tuple] = None):
    self.axes = axes

  def compute(self, a):
    return array_api.sum(a, axes=self.axes)</code>
</pre>
              </li>
            </ul>


            <h1>Reference</h1>
            <p>[1] xx要努力. (2022, 11). Deep Learning System-Homework1. <i>知乎专栏</i>. <a href="https://zhuanlan.zhihu.com/p/579465666">https://zhuanlan.zhihu.com/p/579465666</a>.</p>

            <h1>Contact</h1>
            <p>There may be some errors present. If you find any, please feel free to contact me at <code>wangqiyao@mail.dlut.edu.cn</code>. I would appreciate it!</p>

          </section>
        </div>
      </article>
    </div>
    {% include scripts.html %}
    <script>
        // 动态生成目录
        const toc = document.getElementById('toc');
        toc.innerHTML = '<strong>Table of Contents</strong>';

        const ul = document.createElement('ul');
        toc.appendChild(ul);

        // 获取所有标题
        const headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
        headers.forEach(header => {
            const li = document.createElement('li');
            li.style.marginLeft = `${(parseInt(header.tagName[1]) - 1) * 10}px`;

            const a = document.createElement('a');
            a.href = `#${header.id || header.innerText.replace(/\s+/g, '-').toLowerCase()}`;
            a.textContent = header.innerText;
            a.target="_self";

            if (!header.id) {
                header.id = header.innerText.replace(/\s+/g, '-').toLowerCase();
            }

            li.appendChild(a);
            ul.appendChild(li);
        });
    </script>
  </body>
</html>
