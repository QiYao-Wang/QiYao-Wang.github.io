---
layout: compress
---
<!doctype html>
<html lang="en" class="no-js">

  <head>
    {% include head.html %}
    {% include head/custom.html %}
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
      html {
            scroll-behavior: smooth; /* 启用平滑滚动 */
        }
    </script>
  </head>
  <body>
    {% include browser-upgrade.html %}
    <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="https://qiyao-wang.github.io/">Homepage</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/blogs/">Blogs</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/paper_daily/">Paper Daily</a></li>
          </ul>
        </nav>
      </div>
    </div>
  </div>
    <div id="main" role="main">

      <div class="sidebar sticky">
        <div id="toc"></div>
      </div>

      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        {% if page.title %}<meta itemprop="headline" content="{{ page.title | markdownify | strip_html | strip_newlines | escape_once }}">{% endif %}
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">

            <h1 id="Autoregressive_Decoding">Autoregressive Decoding: Basic Manner of Decoder-Only LLMs</h1>
            <p style="color:gray">Feb. 25, 2025 · Qiyao Wang · Topic #Decoding</p>
            <p>Transformers 架构的语言模型主要包含两大类，即以 BERT 为代表的 Encoder-Only 的 LM 和以 GPT 为代表的 Decoder-Only 的 LM。当前的 LLM 以 Decoder-Only 架构作为其骨架，在此基础上，以自回归（Autoregressive）的方式以 Next Token Prediction 任务在大规模语料上进行预训练。</p>
            <p>自回归机制简单来说，即模型无法看到当前时间步 timestep 之后的数据内容，而是需要以预测的形式向后延伸。在自回归解码中，大模型的最后一层 Transformer 层的 MLP 的维度与词表大小 $|V|$ 相一致，该层 MLP 则用来表征最后的预测词表分布，此时的 MLP 的原始输出被称为 logits。在 logits 的基础上，应用 softmax 函数，得到词表中各个词语的离散概率分布。解码策略此时会起作用，通过从词表的离散概率分布中进行采样，选择最后输出的下一个 token。</p>
            <p>给定序列 $\mathbf{x} = (x_1,x_2,\dots,x_T)$，在时间步 timestep $t$ 时，模型仅可见 $x_1,x_2,...,x_{t-1}$ 这些 token，而 $x_{t},...,x_T$ 不可见，通过 attention mask 进行掩码。以公式的形式表示自回归模型的计算方式，如下式所示</p>
            $$
            \begin{aligned}
            p(\mathbf{x})&=p(x_1)\cdot p(x_2\mid x_1)\cdots p(x_T\mid x_{T-1},x_{T-2},...,x_{1})\\
            &=\prod_{t=1}^Tp(x_t\mid x_{< t})
            \end{aligned}
            $$

            <h2>deco repository</h2>
            <p>Note: All codes will be uploaded at <a href="https://github.com/QiYao-Wang/deco">deco</a>.</p>

            <p>所有的代码将基于 torch 和 transformers 进行实现，其中 python 版本为 3.10，可以通过 `pip install vllm==0.7.3` 来快速构建相关依赖。</p>
            
            <h2>Code</h2>

            <h1>Reference</h1>

            <h1>Contact</h1>
            <p>There may be some errors present. If you find any, please feel free to contact me at <code>wangqiyao@mail.dlut.edu.cn</code>. I would appreciate it!</p>

          </section>
        </div>
      </article>
    </div>
    {% include scripts.html %}
    <script>
    // 动态生成目录
    const toc = document.getElementById('toc');
    toc.innerHTML = '<strong>Table of Contents</strong>';

    const ul = document.createElement('ul');
    toc.appendChild(ul);

    // 获取所有标题
    const headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
    headers.forEach(header => {
        // 忽略 h1，不添加到目录中
        if (header.tagName === 'H1') return;

        const li = document.createElement('li');
        li.style.marginLeft = `${(parseInt(header.tagName[1]) - 1) * 10}px`;

        const a = document.createElement('a');
        a.href = `#${header.id || header.innerText.replace(/\s+/g, '-').toLowerCase()}`;
        a.textContent = header.innerText;
        a.target = "_self";

        // 如果是 h2，使用黑色实心点
        if (header.tagName === 'H2') {
            li.style.listStyleType = 'disc';  // 黑色实心圆点
            a.style.color = 'black';  // h2 为黑色
        }
        // 如果是 h3，使用空心圆点
        else if (header.tagName === 'H3') {
            li.style.listStyleType = 'circle';  // 空心圆点
            a.style.color = 'gray';  // h3 为灰色
        }
        // 如果是 h4，使用方块
        else if (header.tagName === 'H4') {
            li.style.listStyleType = 'square';  // 方块
            a.style.color = 'gray';  // h4 为灰色
        }
        // 如果是 h5 和 h6，使用方块
        else {
            li.style.listStyleType = 'square';  // 方块
            a.style.color = 'gray';  // h5 和 h6 为灰色
        }

        if (!header.id) {
            header.id = header.innerText.replace(/\s+/g, '-').toLowerCase();
        }

        li.appendChild(a);
        ul.appendChild(li);
    });
</script>
  </body>
</html>
