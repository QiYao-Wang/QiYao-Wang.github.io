---
layout: compress
---
<!doctype html>
<html lang="en" class="no-js">
  
  <head>
    {% include head.html %}
    {% include head/custom.html %}
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
      html {
            scroll-behavior: smooth; /* 启用平滑滚动 */
        }
    </script>
  </head>
  <body>
    {% include browser-upgrade.html %}
    <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="https://qiyao-wang.github.io/">Homepage</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/blogs/">Blogs</a></li>
            <li class="masthead__menu-item"><a href="https://qiyao-wang.github.io/paper_daily/">Paper Daily</a></li>
          </ul>
        </nav>
      </div>
    </div>
  </div>
    <div id="main" role="main">

      <div class="sidebar sticky">
        <div id="toc"></div>
      </div>

      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        {% if page.title %}<meta itemprop="headline" content="{{ page.title | markdownify | strip_html | strip_newlines | escape_once }}">{% endif %}
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <h1>CS336 Assignment 1: Transformers Language Model Architecture</h1>
            <p>Feb. 03, 2026 · Qiyao Wang</p>
            
            <div class="blog-content">
        
        <h2 id="transformer-basics">Transformer Basics</h2>
        <p>语言模型的输入是经过 tokenizer 分词后的整数 token IDs 序列 (batch化)，形状为 <code>(batch_size, sequence_length)</code>，输出为正则化的词典中 token 的概率分布，形状为<code>(batch_size, sequence_length, vocab_size)</code>。训练语言模型时，计算输出概率分布和实际标签分布之间的交叉熵损失（cross-entropy loss）。在推理阶段，则使用当前时间步的下一个 token 的分布来采样解码得到预测 token，之后该 token 又会加入输入序列中继续预测下一时间步的 token。</p>

        <p>基于Transformer架构的语言模型主要包括以下几个组件和步骤（见图<a href="#fig1-and-2" target="_self">1</a>）：给定 token IDs 序列，经过 <strong>Embedding Layer</strong> 将整数的 token ID 映射成稠密的向量，之后传入 <code>num_layers</code> 层的 Transformer blocks，取最后一层 Transformer block 的输出，通过一个 FFN（一般称为 LM head）来产生词典级别的输出 token 概率分布，之后进行解码采样得到预测的 token。</p>

        <figure id="fig1-and-2">
          <img src="/blogs/2026/CS336/Assignment1/figure1-and-2.png" alt="Transformer">
          <figcaption>图1：(Figure 1) Transformer 架构图; (Figure 2) Pre-norm Transformer Block.</figcaption>
        </figure>

        <h3 id="efficient-computation">Efficient Computation</h3>
        <p>Transformer 架构中一般进行 batch-level 的并行计算。在具体分析的时候主要关注下面三个元素：
          <ul>  
            <li><strong>Elements of a batch</strong>: 每一次 forward computation 在一个 batch_size 大小的数据中进行，<code>forward</code>依次作用到 batch 中的每一个数据中。</li>
            <li><strong>Sequence length</strong>: 对于 RMSNorm 和 Feed-forward 等 point-wise 的位置无关操作，在序列中每个位置的处理都一样。</li>
            <li><strong>Attention heads</strong>: 注意力操作可以并行，也是 Transformer 可以并行的基础之一，并且可以基于 batch 来处理多头注意力操作。</li>
          </ul>
          举例：data tensor D 的形状为 <code>(batch_size, sequence_length, d_model)</code>，在其上进行 batched vector-matrix multiply，矩阵为 A，形状是 <code>(d_model, d_model)</code>，则 <code>D @ A</code> 会进行 batched matrix multiply，其中 <code>(batch_size, sequence_length)</code> 是批处理的。
        </p>
        <p>使用 <strong>einsum notation</strong> 进行更方便的批处理。PyTorch 中是 <code>torch.einsum</code>，与框架无关的库是 <code>einops</code> （本次使用）和 <code>einx</code>。</p>
        <p>主要有两个关键的操作：
          <ul>
            <li><strong>einsum</strong>: 可以对 tensor 做任意的操作，见下面的表格的总结。</li>
            <table border="1" cellspacing="0" cellpadding="6">
  <thead>
    <tr>
      <th>操作类型</th>
      <th>einsum 表达式示例</th>
      <th>张量形状示例</th>
      <th>等价常见操作</th>
      <th>说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>转置 / 维度重排</td>
      <td><code>einsum("ij->ji", A)</code></td>
      <td>A: (i, j)</td>
      <td><code>A.T</code>, <code>permute</code></td>
      <td>只改变维度顺序，不做数值计算</td>
    </tr>
    <tr>
      <td>求和（Reduce）</td>
      <td><code>einsum("ij->i", A)</code></td>
      <td>A: (i, j)</td>
      <td><code>A.sum(dim=1)</code></td>
      <td>消除未出现在输出中的维度</td>
    </tr>
    <tr>
      <td>逐元素乘法</td>
      <td><code>einsum("ij,ij->ij", A, B)</code></td>
      <td>A,B: (i, j)</td>
      <td><code>A * B</code></td>
      <td>对应元素相乘</td>
    </tr>
    <tr>
      <td>内积 / 点积</td>
      <td><code>einsum("i,i->", x, y)</code></td>
      <td>x,y: (i)</td>
      <td><code>torch.dot</code></td>
      <td>输出标量</td>
    </tr>
    <tr>
      <td>外积</td>
      <td><code>einsum("i,j->ij", x, y)</code></td>
      <td>x:(i), y:(j)</td>
      <td><code>outer</code></td>
      <td>生成高阶张量</td>
    </tr>
    <tr>
      <td>矩阵乘法</td>
      <td><code>einsum("ij,jk->ik", A, B)</code></td>
      <td>A:(i,j), B:(j,k)</td>
      <td><code>matmul</code></td>
      <td>经典线性代数操作</td>
    </tr>
    <tr>
      <td>批量矩阵乘</td>
      <td><code>einsum("bij,bjk->bik", A, B)</code></td>
      <td>A,B:(b,i,j)</td>
      <td><code>bmm</code></td>
      <td>每个 batch 独立计算</td>
    </tr>
    <tr>
      <td>Trace（迹）</td>
      <td><code>einsum("ii->", A)</code></td>
      <td>A:(i,i)</td>
      <td><code>trace</code></td>
      <td>对角线求和</td>
    </tr>
    <tr>
      <td>Attention 核心计算</td>
      <td><code>einsum("bhtd,bhsd->bhts", Q, K)</code></td>
      <td>Q,K:(b,h,t,d)</td>
      <td>Q @ Kᵀ</td>
      <td>Transformer 中的相似度计算</td>
    </tr>
    <tr>
      <td>线性层 / Token Mixing</td>
      <td><code>einsum("btc,cd->btd", X, W)</code></td>
      <td>X:(b,t,c), W:(c,d)</td>
      <td>Linear / MatMul</td>
      <td>特征维度线性变换</td>
    </tr>
    <tr>
      <td>多操作融合</td>
      <td><code>einsum("bct,td->bc", X, W)</code></td>
      <td>X:(b,c,t)</td>
      <td>permute + matmul + sum</td>
      <td>一行完成多步计算</td>
    </tr>
  </tbody>
</table>
        <li><strong>rearrange</strong>: 可以重新排列、连接和拆分张量的维度。</li>
        <table border="1" cellspacing="0" cellpadding="6">
  <thead>
    <tr>
      <th>操作类型</th>
      <th>rearrange 表达式示例</th>
      <th>输入形状示例</th>
      <th>输出形状</th>
      <th>等价常见操作</th>
      <th>说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>转置 / 维度重排</td>
      <td><code>rearrange(x, "b c h w -> b h w c")</code></td>
      <td>(b, c, h, w)</td>
      <td>(b, h, w, c)</td>
      <td><code>permute</code></td>
      <td>仅改变维度顺序，不改数值</td>
    </tr>
    <tr>
      <td>合并维度（merge）</td>
      <td><code>rearrange(x, "b t d -> b (t d)")</code></td>
      <td>(b, t, d)</td>
      <td>(b, t·d)</td>
      <td><code>reshape</code></td>
      <td>将多个维度合并成一个</td>
    </tr>
    <tr>
      <td>拆分维度（split）</td>
      <td><code>rearrange(x, "b (h d) -> b h d", h=8)</code></td>
      <td>(b, h·d)</td>
      <td>(b, h, d)</td>
      <td><code>view</code></td>
      <td>需要显式指定拆分尺寸</td>
    </tr>
    <tr>
      <td>Flatten</td>
      <td><code>rearrange(x, "b c h w -> b (c h w)")</code></td>
      <td>(b, c, h, w)</td>
      <td>(b, c·h·w)</td>
      <td><code>flatten</code></td>
      <td>常用于 CNN → Transformer</td>
    </tr>
    <tr>
      <td>Unflatten</td>
      <td><code>rearrange(x, "b (h w) c -> b c h w", h=32)</code></td>
      <td>(b, h·w, c)</td>
      <td>(b, c, h, w)</td>
      <td><code>view</code></td>
      <td>恢复空间结构</td>
    </tr>
    <tr>
      <td>Patch 划分（ViT）</td>
      <td><code>rearrange(x, "b c (h p1) (w p2) -> b (h w) (c p1 p2)", p1=16, p2=16)</code></td>
      <td>(b, c, H, W)</td>
      <td>(b, num_patches, patch_dim)</td>
      <td>手写切块</td>
      <td>Vision Transformer 核心操作</td>
    </tr>
    <tr>
      <td>多头拆分（Heads）</td>
      <td><code>rearrange(x, "b t (h d) -> b h t d", h=8)</code></td>
      <td>(b, t, h·d)</td>
      <td>(b, h, t, d)</td>
      <td>reshape + permute</td>
      <td>Attention 预处理</td>
    </tr>
    <tr>
      <td>多头合并</td>
      <td><code>rearrange(x, "b h t d -> b t (h d)")</code></td>
      <td>(b, h, t, d)</td>
      <td>(b, t, h·d)</td>
      <td>reshape</td>
      <td>Attention 输出恢复</td>
    </tr>
    <tr>
      <td>Batch / Token 混合</td>
      <td><code>rearrange(x, "b t d -> (b t) d")</code></td>
      <td>(b, t, d)</td>
      <td>(b·t, d)</td>
      <td>view</td>
      <td>常用于共享线性层</td>
    </tr>
    <tr>
      <td>保持不变（Identity）</td>
      <td><code>rearrange(x, "b t d -> b t d")</code></td>
      <td>(b, t, d)</td>
      <td>(b, t, d)</td>
      <td>无</td>
      <td>用于代码可读性 / 占位</td>
    </tr>
  </tbody>
</table>    
      </ul>
        </p>

<pre><code class="language-python">import torch
from einops import einsum, rearrange

def einstein_example1():
    D = torch.rand(5, 3, 4)
    A = torch.rand(3, 4)

    # 普通矩阵乘法，不知道 D 和 A 具体的形状以及输出的形状
    Y = D @ A.T

    # 使用 einsum 进行矩阵乘法，可读性更好
    Y = einsum(D, A, "batch sequence d_in, d_out d_in -> batch sequence d_out")

    # D 可以有任意的前置维度，A 受限
    Y = einsum(D, A, "... d_in, d_out d_in -> ... d_out")</code></pre>

        <p>Token Embeddings: <code>(batch_size, sequence_length)</code> $\to$ <code>(batch_size, sequence_length, d_model)</code>.</p>
        <p>发生在每个Transformer Block 中的 Attention 计算之前的 Normalization，Pre-norm Transformer Block: <code>(batch_size, sequence_length, d_model)</code> $\to$ <code>(batch_size, sequence_length, d_model).</code></p>
        <p>Output Normalization 会在最后一个 Transformer Block 后进行 Layer Normalization。</p>


        <h2 id="reference">Reference</h2>
        <p>[1] <a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf">CS336 Assignment 1 (basics): Building a Transformer LM. Version 1.0.5</a></p>
        <h2 id="contact">Contact</h2>
        <p>There may be some errors present. If you find any, please feel free to contact me at <code>wangqiyao@mail.dlut.edu.cn</code>. I would appreciate it!</p>
      </div>
    </div>
  </div>

            
            
          </section>
        </div>
      </article>
    </div>
    {% include scripts.html %}
    <script>
    // 动态生成目录
    const toc = document.getElementById('toc');
    toc.innerHTML = '<strong>Table of Contents</strong>';

    const ul = document.createElement('ul');
    toc.appendChild(ul);

    // 获取所有标题
    const headers = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
    headers.forEach(header => {
        // 忽略 h1，不添加到目录中
        if (header.tagName === 'H1') return;

        const li = document.createElement('li');
        li.style.marginLeft = `${(parseInt(header.tagName[1]) - 1) * 10}px`;

        const a = document.createElement('a');
        a.href = `#${header.id || header.innerText.replace(/\s+/g, '-').toLowerCase()}`;
        a.textContent = header.innerText;
        a.target = "_self";

        // 如果是 h2，使用黑色实心点
        if (header.tagName === 'H2') {
            li.style.listStyleType = 'disc';  // 黑色实心圆点
            a.style.color = 'black';  // h2 为黑色
        }
        // 如果是 h3，使用空心圆点
        else if (header.tagName === 'H3') {
            li.style.listStyleType = 'circle';  // 空心圆点
            a.style.color = 'gray';  // h3 为灰色
        }
        // 如果是 h4，使用方块
        else if (header.tagName === 'H4') {
            li.style.listStyleType = 'square';  // 方块
            a.style.color = 'gray';  // h4 为灰色
        }
        // 如果是 h5 和 h6，使用方块
        else {
            li.style.listStyleType = 'square';  // 方块
            a.style.color = 'gray';  // h5 和 h6 为灰色
        }

        if (!header.id) {
            header.id = header.innerText.replace(/\s+/g, '-').toLowerCase();
        }

        li.appendChild(a);
        ul.appendChild(li);
    });
</script>
  </body>
</html>